{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62dd4f26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the following will make jupyter display 'wider' (less side-to-side scrolling required)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37b5718f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,OneHotEncoder\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb95480",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a2b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are lots of empty entries in the 'Bean_Type' column.\n",
    "# They don't hurt anything, but can be confusing in any output.\n",
    "def blank_to_unspecified(value):\n",
    "    value = value.strip()\n",
    "    if value == '':\n",
    "        value = 'unspecified'\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb04d3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a hack. I'm not positive, but I'm pretty sure\n",
    "# that Neural Networks must have a binary Target field.\n",
    "# So this is my way of identifying any ratings which fall\n",
    "# within the desired range.\n",
    "#\n",
    "# return True if rating is >= 400\n",
    "# else return False\n",
    "#\n",
    "# Note that when I tried using actual bool return types (i.e. True & False)\n",
    "# I got some wierd errors. This probably needs improvement. It would also\n",
    "# be cool if we could make this more configurable, so the user could\n",
    "# create different neural nets that look for different ranges of 'Rating'\n",
    "def bin_ratings(rating):\n",
    "    if rating == 5.75: return 1\n",
    "    if rating == 5.50: return 1\n",
    "    if rating == 5.25: return 1\n",
    "    if rating == 5.00: return 1\n",
    "    \n",
    "    if rating == 4.75: return 1\n",
    "    if rating == 4.50: return 1\n",
    "    if rating == 4.25: return 1\n",
    "    if rating == 4.00: return 1\n",
    "    \n",
    "    if rating == 3.75: return 0\n",
    "    if rating == 3.50: return 0\n",
    "    if rating == 3.25: return 0\n",
    "    if rating == 3.00: return 0\n",
    "\n",
    "    if rating == 2.75: return 0\n",
    "    if rating == 2.50: return 0\n",
    "    if rating == 2.25: return 0\n",
    "    if rating == 2.00: return 0\n",
    "    \n",
    "    if rating == 1.75: return 0\n",
    "    if rating == 1.50: return 0\n",
    "    if rating == 1.25: return 0\n",
    "    if rating == 1.00: return 0\n",
    "    \n",
    "    #print( f\"error: rating={rating} type={type(rating)}\" )\n",
    "    return \"2\"\n",
    "\n",
    "\n",
    "#     value = int(float(rating) * 100)\n",
    "#     if(value >= 400):\n",
    "#         return \"1\"\n",
    "#     else:\n",
    "#         return \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e0d1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function performs the fit_transform step using whichever\n",
    "# scaler object is passed in (e.g. StandardScaler or MinMaxScaler)\n",
    "def do_scale(scaler, X_train, X_test):\n",
    "    # Scale the data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a32e99d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine which values to replace if counts are less than 'threshold'\n",
    "def reduce_count_vals(df, colname, threshold):\n",
    "    counts = df[colname].value_counts()\n",
    "    replace_list = list(counts[counts < threshold].index)\n",
    "\n",
    "    # Replace in dataframe\n",
    "    for item in replace_list:\n",
    "       df[colname] = df[colname].replace(item,\"Other\")\n",
    "    \n",
    "    # Check to make sure binning was successful\n",
    "    #df[colname].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "863dc21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function automatically finds the columns\n",
    "# which are objects (since they are most likely 'strings')\n",
    "# does the following:\n",
    "# - gets the column names\n",
    "# - instantiates a OneHotEncoder object\n",
    "# - creates a new dataframe with just the column names from above\n",
    "# - encodes just those features\n",
    "# - merges the new one-hot-encoded columns back into the original dataframe\n",
    "# - drops the original column names from the dataframe\n",
    "# - createa a pd.Series from the 'Target' column\n",
    "# - drops the target column\n",
    "# - creates the X features\n",
    "# - returns the munged dataframe (df), the features (X), and the target (y)\n",
    "def do_one_hot(df, y_col):\n",
    "    # Create a list of columns that are 'object' type\n",
    "    obj_cat = df.dtypes[df.dtypes == \"object\"].index.tolist()\n",
    "    # Create a OneHotEncoder instance\n",
    "    enc = OneHotEncoder(sparse=False)\n",
    "    # Fit and transform the OneHotEncoder using the categorical variable list\n",
    "    encode_df = pd.DataFrame(enc.fit_transform(df[obj_cat]))\n",
    "    # Add the encoded variable names to the DataFrame\n",
    "    encode_df.columns = enc.get_feature_names(obj_cat)\n",
    "    \n",
    "    # Merge one-hot encoded features and drop the originals\n",
    "    df = df.merge(encode_df, left_index=True, right_index=True)\n",
    "    df = df.drop(obj_cat,1)\n",
    "    \n",
    "    # Split our preprocessed data into our features and target arrays\n",
    "    y = df[y_col].values\n",
    "    df.drop(columns=[y_col], inplace=True)\n",
    "    X = df.values\n",
    "    print(X)\n",
    "    print(f\"merged df.shape()={df.shape}\")\n",
    "    \n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9abb616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function automatically creates the Neural Network\n",
    "# input, hidden, and output layers based on parameters\n",
    "# stored in the 'layers' dictionary\n",
    "def build_model(inputs, layers):\n",
    "    nn = tf.keras.models.Sequential()\n",
    "    first = True\n",
    "    for layer in layers:\n",
    "        if first:\n",
    "            first = False\n",
    "            nn.add(tf.keras.layers.Dense(units=layer['units'], activation=layer['act'], input_dim=inputs))\n",
    "        else:\n",
    "            nn.add(tf.keras.layers.Dense(units=layer['units'], activation=layer['act']))\n",
    "\n",
    "    print(nn.summary())\n",
    "    nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2be20fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will automatically perform splitting and training\n",
    "# of the the Neural Net model. It returns 'fit_model' in case\n",
    "# it's needed for later steps in the flow\n",
    "#\n",
    "# FIXME: There's a bug in here somewhere. I can't use the 'checkpoints_dir'\n",
    "# parameter below, because the '{epoch:02d}.hdf5' stuff dies if the line\n",
    "# of code looks like this:\n",
    "#        checkpoint_path = f\"{checkpoints_dir}/weights_2.{epoch:02d}.hdf5\"\n",
    "# something about the first pair of curly braces vs. the second pair\n",
    "# inside the f-string. So I just hard-coded the dir to always be 'checkpoints'\n",
    "\n",
    "def train_nn_model(model, X_train, y_train, n_epochs, checkpoint_dir):\n",
    "    # Create a callback that saves the model's weights every epoch\n",
    "    from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "    \n",
    "    # Define the checkpoint path and filenames\n",
    "    os.makedirs(\"checkpoints\",exist_ok=True)\n",
    "    #checkpoint_file = f\"weights.{epoch:02d}.hdf5\"\n",
    "    #checkpoint_path = f\"checkpoints_opt/weights.{epoch:02d}.hdf5\"\n",
    "    checkpoint_path = \"checkpoints/weights_2.{epoch:02d}.hdf5\"\n",
    "    \n",
    "    cp_callback_opt = ModelCheckpoint(\n",
    "        filepath=checkpoint_path,\n",
    "        verbose=1,\n",
    "        save_weights_only=True,\n",
    "        period=5)\n",
    "\n",
    "    # Normally we use 'save_freq', but it behaves strangely, and I could not get\n",
    "    # it to save every 5 epochs. The 'period' param is now deprecated, but it works.\n",
    "        #save_freq='epoch')\n",
    "        \n",
    "    fit_model = model.fit(X_train, y_train, epochs=n_epochs, callbacks=[cp_callback_opt])\n",
    "    return fit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca6fbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is no longer used, since I now drop 'Company' and 'Bean_Origin_or_Bar_Name',\n",
    "def bin_names(name,map,counter):\n",
    "    if name in map:\n",
    "        return map[name]\n",
    "    else:\n",
    "        counter[0] = counter[0] + 1\n",
    "        map[name] = counter[0]\n",
    "        return map[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8f8aca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a hack to reduce the number of entries in the 'Cocoa_Percent' column.\n",
    "def bin_percentiles(percentile):\n",
    "    percentile = int(float(percentile.replace('%', '')))\n",
    "    if(percentile >= 95): return \"95\"\n",
    "    if(percentile >= 90): return \"90\"\n",
    "    if(percentile >= 85): return \"85\"\n",
    "    if(percentile >= 80): return \"80\"\n",
    "    if(percentile >= 75): return \"75\"\n",
    "    if(percentile >= 70): return \"70\"\n",
    "    if(percentile >= 65): return \"65\"\n",
    "    if(percentile >= 60): return \"60\"\n",
    "    if(percentile >= 55): return \"55\"\n",
    "    return \"50_or_below\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8efc32a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The scatter plots are mildly interesting, but any 'size' other than\n",
    "# the default of '1' makes it too messy\n",
    "def do_scatter_plots(df):\n",
    "    #fig, axes = plt.subplots(nrows=8, ncols=8, figsize=(24,24))\n",
    "    colnames = df.columns\n",
    "    ncols = len(colnames)\n",
    "    for xx in range(0,ncols):\n",
    "        for yy in range(xx+1,ncols):\n",
    "            #if xx == yy:\n",
    "            #    continue\n",
    "            titlestring = f\"x={colnames[xx]} vs. y={colnames[yy]}\"\n",
    "            #df.plot.scatter(ax=axes[xx,yy], x=colnames[xx], y=colnames[yy], title=titlestring, c=df['IS_SUCCESSFUL'], s=(df['ASK_AMT']*20), colormap='winter')\n",
    "            df.plot.scatter(\n",
    "                figsize=(6,6),\n",
    "                x=colnames[xx], \n",
    "                y=colnames[yy], \n",
    "                title=titlestring,\n",
    "                xlabel=colnames[xx],\n",
    "                ylabel=colnames[yy],\n",
    "                c=df['Rating'], \n",
    "                colormap='winter'\n",
    "            )\n",
    "                #s=(df['Rating']), \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8623093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some reason, I kept getting errors in one-hot encoding unless I \n",
    "# forced 'Review Date' to be interpreted as a string (even though it was\n",
    "# a string after the read_csv() step - FIXME)\n",
    "def to_string(value):\n",
    "    other = f\"_{value}_\"\n",
    "    return other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bd315e",
   "metadata": {},
   "source": [
    "# Extract, Transform, Load stuff (but the Load into a DB piece is missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee115f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ETL():\n",
    "    df = pd.read_csv(\"Resources/flavors_of_cacao.csv\")\n",
    "\n",
    "    # replace ' ' with '_' in column names\n",
    "    colnames = [\n",
    "            'Company',\n",
    "            'Bean_Origin_or_Bar_Name',\n",
    "            'REF',\n",
    "            'Review_Date',\n",
    "            'Cocoa_Percent',\n",
    "            'Company_Location',\n",
    "            'Rating',\n",
    "            'Bean_Type',\n",
    "            'Broad_Bean_Origin'\n",
    "    ]\n",
    "    df.columns = colnames\n",
    "\n",
    "    df.dropna(axis='rows', how='any', inplace=True)\n",
    "\n",
    "    company_names = {}\n",
    "    company_counter = [0]\n",
    "    origin_names = {}\n",
    "    origin_counter = [0]\n",
    "\n",
    "    #df['Company']           = df['Company'].apply(bin_names, args=(company_names, company_counter))\n",
    "    #df['BeanBarName']       = df['BeanBarName'].apply(bin_names, args=(origin_names, origin_counter))\n",
    "    df.drop(columns=[\n",
    "        'Company',\n",
    "        'Bean_Origin_or_Bar_Name',\n",
    "        'REF',\n",
    "        'Company_Location'\n",
    "    ], inplace=True)\n",
    "\n",
    "    df['Review_Date']       = df['Review_Date'].apply(to_string)\n",
    "    reduce_count_vals(df, 'Review_Date', 100)\n",
    "\n",
    "    df['Cocoa_Percent']     = df['Cocoa_Percent'].apply(bin_percentiles)\n",
    "\n",
    "    #reduce_count_vals(df, 'Company_Location', 38)\n",
    "\n",
    "    df['Rating']            = df['Rating'].apply(bin_ratings)\n",
    "\n",
    "    df['Bean_Type']         = df['Bean_Type'].apply(blank_to_unspecified)\n",
    "    reduce_count_vals(df, 'Bean_Type', 10)\n",
    "\n",
    "    reduce_count_vals(df, 'Broad_Bean_Origin', 40)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9faa0f",
   "metadata": {},
   "source": [
    "# Create and Train the Neural Network stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "966783b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_NN(df):\n",
    "    df, X, y = do_one_hot(df, 'Rating')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=78)\n",
    "    \n",
    "    print(f\"X_train={X_train}\")\n",
    "    print(f\"y_train={y_train}\")\n",
    "\n",
    "    #X_train_scaled, X_test_scaled = do_scale(StandardScaler(), X_train, X_test)\n",
    "    X_train_scaled, X_test_scaled = do_scale(MinMaxScaler(), X_train, X_test)\n",
    "\n",
    "    nins = df.shape[1]\n",
    "    print(f\"nins={nins}\")\n",
    "    nn_model = build_model(inputs=nins, layers=[\n",
    "        #{'units': 512, 'act': 'relu'},\n",
    "        {'units': 256, 'act': 'relu'},\n",
    "        {'units': 128, 'act': 'relu'},\n",
    "        {'units': 64, 'act': 'relu'},\n",
    "        {'units': 32, 'act': 'relu'},\n",
    "        {'units': 16, 'act': 'relu'},\n",
    "        {'units': 8, 'act': 'relu'},\n",
    "        {'units': 4, 'act': 'relu'},\n",
    "        {'units': 1, 'act': 'sigmoid'},\n",
    "\n",
    "    ])\n",
    "    trained_model = train_nn_model(nn_model, X_train_scaled, y_train, n_epochs=100, checkpoint_dir=\"checkpoints\")\n",
    "\n",
    "    model_loss, model_accuracy = nn_model.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "    print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n",
    "\n",
    "    nn_model.save(\"ChocoloateBarRatings_opt1.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ba643",
   "metadata": {},
   "source": [
    "# Do the actual work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "374bb13b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]]\n",
      "merged df.shape()=(1791, 42)\n",
      "X_train=[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "y_train=[0 0 0 ... 0 0 0]\n",
      "nins=42\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               11008     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 54,945\n",
      "Trainable params: 54,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bjile\\anaconda3\\envs\\mlenv\\lib\\site-packages\\ipykernel_launcher.py:26: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "42/42 [==============================] - 1s 2ms/step - loss: 0.6602 - accuracy: 0.9017\n",
      "Epoch 2/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9434\n",
      "Epoch 3/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.2105 - accuracy: 0.9434\n",
      "Epoch 4/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1988 - accuracy: 0.9434\n",
      "Epoch 5/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1938 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00005: saving model to checkpoints\\weights_2.05.hdf5\n",
      "Epoch 6/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1891 - accuracy: 0.9434\n",
      "Epoch 7/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1913 - accuracy: 0.9434\n",
      "Epoch 8/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1831 - accuracy: 0.9434\n",
      "Epoch 9/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1797 - accuracy: 0.9434\n",
      "Epoch 10/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00010: saving model to checkpoints\\weights_2.10.hdf5\n",
      "Epoch 11/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1696 - accuracy: 0.9434\n",
      "Epoch 12/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.9434\n",
      "Epoch 13/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1592 - accuracy: 0.9434\n",
      "Epoch 14/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9434\n",
      "Epoch 15/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1440 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00015: saving model to checkpoints\\weights_2.15.hdf5\n",
      "Epoch 16/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1402 - accuracy: 0.9434\n",
      "Epoch 17/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1282 - accuracy: 0.9434\n",
      "Epoch 18/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1212 - accuracy: 0.9434\n",
      "Epoch 19/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1223 - accuracy: 0.9434\n",
      "Epoch 20/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00020: saving model to checkpoints\\weights_2.20.hdf5\n",
      "Epoch 21/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1158 - accuracy: 0.9434\n",
      "Epoch 22/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1125 - accuracy: 0.9434\n",
      "Epoch 23/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9434\n",
      "Epoch 24/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9434\n",
      "Epoch 25/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1063 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00025: saving model to checkpoints\\weights_2.25.hdf5\n",
      "Epoch 26/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1032 - accuracy: 0.9434\n",
      "Epoch 27/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1022 - accuracy: 0.9434\n",
      "Epoch 28/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1046 - accuracy: 0.9434\n",
      "Epoch 29/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1051 - accuracy: 0.9434\n",
      "Epoch 30/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9434\n",
      "\n",
      "Epoch 00030: saving model to checkpoints\\weights_2.30.hdf5\n",
      "Epoch 31/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1007 - accuracy: 0.9434\n",
      "Epoch 32/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.9442\n",
      "Epoch 33/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1005 - accuracy: 0.9553\n",
      "Epoch 34/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1009 - accuracy: 0.9568\n",
      "Epoch 35/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1011 - accuracy: 0.9546\n",
      "\n",
      "Epoch 00035: saving model to checkpoints\\weights_2.35.hdf5\n",
      "Epoch 36/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1010 - accuracy: 0.9583\n",
      "Epoch 37/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1001 - accuracy: 0.9576\n",
      "Epoch 38/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.9546\n",
      "Epoch 39/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0984 - accuracy: 0.9568\n",
      "Epoch 40/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00040: saving model to checkpoints\\weights_2.40.hdf5\n",
      "Epoch 41/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1001 - accuracy: 0.9538\n",
      "Epoch 42/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9553\n",
      "Epoch 43/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0987 - accuracy: 0.9561\n",
      "Epoch 44/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0977 - accuracy: 0.9576\n",
      "Epoch 45/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00045: saving model to checkpoints\\weights_2.45.hdf5\n",
      "Epoch 46/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0983 - accuracy: 0.9561\n",
      "Epoch 47/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9568\n",
      "Epoch 48/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9568\n",
      "Epoch 49/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9568\n",
      "Epoch 50/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9568\n",
      "\n",
      "Epoch 00050: saving model to checkpoints\\weights_2.50.hdf5\n",
      "Epoch 51/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9576\n",
      "Epoch 52/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0972 - accuracy: 0.9561\n",
      "Epoch 53/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9561\n",
      "Epoch 54/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9553\n",
      "Epoch 55/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0985 - accuracy: 0.9561\n",
      "\n",
      "Epoch 00055: saving model to checkpoints\\weights_2.55.hdf5\n",
      "Epoch 56/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9576\n",
      "Epoch 57/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0970 - accuracy: 0.9568\n",
      "Epoch 58/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9568\n",
      "Epoch 59/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9576\n",
      "Epoch 60/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0978 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00060: saving model to checkpoints\\weights_2.60.hdf5\n",
      "Epoch 61/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9576\n",
      "Epoch 62/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0962 - accuracy: 0.9576\n",
      "Epoch 63/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9576\n",
      "Epoch 64/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9576\n",
      "Epoch 65/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00065: saving model to checkpoints\\weights_2.65.hdf5\n",
      "Epoch 66/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9576\n",
      "Epoch 67/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9561\n",
      "Epoch 68/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.1040 - accuracy: 0.9576\n",
      "Epoch 69/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9568\n",
      "Epoch 70/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9553\n",
      "\n",
      "Epoch 00070: saving model to checkpoints\\weights_2.70.hdf5\n",
      "Epoch 71/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9576\n",
      "Epoch 72/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9568\n",
      "Epoch 73/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0988 - accuracy: 0.9553\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0982 - accuracy: 0.9561\n",
      "Epoch 75/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9561\n",
      "\n",
      "Epoch 00075: saving model to checkpoints\\weights_2.75.hdf5\n",
      "Epoch 76/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9568\n",
      "Epoch 77/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9553\n",
      "Epoch 78/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9553: 0s - loss: 0.1021 - accuracy: 0.94\n",
      "Epoch 79/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.9576\n",
      "Epoch 80/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0958 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00080: saving model to checkpoints\\weights_2.80.hdf5\n",
      "Epoch 81/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0952 - accuracy: 0.9568\n",
      "Epoch 82/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9568\n",
      "Epoch 83/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9568\n",
      "Epoch 84/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9568\n",
      "Epoch 85/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0956 - accuracy: 0.9576\n",
      "\n",
      "Epoch 00085: saving model to checkpoints\\weights_2.85.hdf5\n",
      "Epoch 86/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9568\n",
      "Epoch 87/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9576\n",
      "Epoch 88/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0949 - accuracy: 0.9576\n",
      "Epoch 89/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0963 - accuracy: 0.9561\n",
      "Epoch 90/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0976 - accuracy: 0.9568\n",
      "\n",
      "Epoch 00090: saving model to checkpoints\\weights_2.90.hdf5\n",
      "Epoch 91/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9546\n",
      "Epoch 92/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0959 - accuracy: 0.9568\n",
      "Epoch 93/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0963 - accuracy: 0.9568: 0s - loss: 0.0997 - accuracy: 0.95\n",
      "Epoch 94/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0960 - accuracy: 0.9561\n",
      "Epoch 95/100\n",
      "42/42 [==============================] - 0s 3ms/step - loss: 0.0958 - accuracy: 0.9561\n",
      "\n",
      "Epoch 00095: saving model to checkpoints\\weights_2.95.hdf5\n",
      "Epoch 96/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9561\n",
      "Epoch 97/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9568\n",
      "Epoch 98/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0954 - accuracy: 0.9561\n",
      "Epoch 99/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0953 - accuracy: 0.9568\n",
      "Epoch 100/100\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0950 - accuracy: 0.9568\n",
      "\n",
      "Epoch 00100: saving model to checkpoints\\weights_2.100.hdf5\n",
      "14/14 - 0s - loss: 0.7793 - accuracy: 0.9286\n",
      "Loss: 0.7792798280715942, Accuracy: 0.9285714030265808\n"
     ]
    }
   ],
   "source": [
    "df = do_ETL()\n",
    "do_NN(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451670b",
   "metadata": {},
   "source": [
    "### How many 'Bean Types' are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7351919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 41 different 'Bean_Type' entries, even though only Criollo, Forastero, and Trinitario species exist\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                            887\n",
       "Trinitario                  419\n",
       "Criollo                     153\n",
       "Forastero                    87\n",
       "Forastero (Nacional)         52\n",
       "Blend                        41\n",
       "Criollo, Trinitario          39\n",
       "Forastero (Arriba)           37\n",
       "Criollo (Porcelana)          10\n",
       "Trinitario, Criollo           9\n",
       "Forastero (Parazinho)         8\n",
       "Forastero (Arriba) ASS        6\n",
       "Nacional (Arriba)             3\n",
       "Matina                        3\n",
       "EET                           3\n",
       "Beniano                       3\n",
       "Criollo (Ocumare 61)          2\n",
       "Trinitario, Forastero         2\n",
       "Trinitario (85% Criollo)      2\n",
       "Forastero (Catongo)           2\n",
       "Criollo, Forastero            2\n",
       "Amazon, ICS                   2\n",
       "Criollo (Amarru)              2\n",
       "Amazon mix                    2\n",
       "Nacional                      2\n",
       "Criollo (Ocumare)             1\n",
       "Criollo (Ocumare 67)          1\n",
       "Trinitario, TCGA              1\n",
       "Trinitario (Amelonado)        1\n",
       "Trinitario, Nacional          1\n",
       "Forastero (Amelonado)         1\n",
       "Forastero, Trinitario         1\n",
       "Forastero (Arriba) ASSS       1\n",
       "Forastero(Arriba, CCN)        1\n",
       "Criollo, +                    1\n",
       "Criollo (Wild)                1\n",
       "Trinitario (Scavina)          1\n",
       "Criollo (Ocumare 77)          1\n",
       "CCN51                         1\n",
       "Amazon                        1\n",
       "Blend-Forastero,Criollo       1\n",
       "Name: Bean_Type, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Resources/flavors_of_cacao.csv\")\n",
    "num_beans = len(df['Bean_Type'].value_counts())\n",
    "print(f\"\\nThere are {num_beans} different 'Bean_Type' entries, even though only Criollo, Forastero, and Trinitario species exist\\n\")\n",
    "df['Bean_Type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3601083c",
   "metadata": {},
   "source": [
    "### How many 'Broad_Bean_Origins' are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41b63e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "There are 100 different 'Broad_Bean_Origin' entries\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Venezuela                214\n",
       "Ecuador                  193\n",
       "Peru                     165\n",
       "Madagascar               145\n",
       "Dominican Republic       141\n",
       "                        ... \n",
       "Peru, Belize               1\n",
       "Peru, Mad., Dom. Rep.      1\n",
       "PNG, Vanuatu, Mad          1\n",
       "Trinidad, Ecuador          1\n",
       "Venezuela, Carribean       1\n",
       "Name: Broad_Bean_Origin, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Resources/flavors_of_cacao.csv\")\n",
    "num_origins = len(df['Broad_Bean_Origin'].value_counts())\n",
    "print(f\"\\nThere are {num_origins} different 'Broad_Bean_Origin' entries\\n\")\n",
    "df['Broad_Bean_Origin'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8cf26",
   "metadata": {},
   "source": [
    "### Scatter plots on original, unmodified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "138fdea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME: this used to work yesterday, before all my data-cleaning hacks. Do not use now.\n",
    "#df = pd.read_csv(\"Resources/flavors_of_cacao.csv\")\n",
    "#do_scatter_plots(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
